# app/services/base_generative_service.py
import hashlib  # <-- NOVA IMPORTAÃ‡ÃƒO para criar chaves de cache estÃ¡veis
import logging
import os

import google.generativeai as genai

from app import cache  # <-- NOVA IMPORTAÃ‡ÃƒO


class BaseGenerativeService:
    def __init__(self, model_name="gemini-1.5-flash-latest"):
        self.logger = logging.getLogger(self.__class__.__name__)
        self.model = None
        self._google_api_key = None

        try:
            # Tenta usar GOOGLE_API_KEY_1 primeiro, depois GOOGLE_API_KEY como fallback
            # IMPORTANTE: Configure no .env ou variÃ¡veis de ambiente:
            # - GOOGLE_API_KEY_1 (API Key principal para inicializaÃ§Ã£o)
            # - GOOGLE_API_KEY_2 (API Key de backup para fallback)
            self._google_api_key = os.getenv("GOOGLE_API_KEY_1") or os.getenv("GOOGLE_API_KEY")
            if not self._google_api_key:
                self.logger.error(
                    "API Key do Google (GOOGLE_API_KEY_1 ou GOOGLE_API_KEY) nÃ£o encontrada nas variÃ¡veis de ambiente."
                )
                raise RuntimeError(
                    "API Key do Google (GOOGLE_API_KEY_1 ou GOOGLE_API_KEY) nÃ£o configurada nas variÃ¡veis de ambiente."
                )

            genai.configure(api_key=self._google_api_key)
            self.logger.info(
                "ConfiguraÃ§Ã£o da API Key do Google bem-sucedida para o serviÃ§o."
            )

            generation_config = {
                "temperature": 0.7,
                "top_p": 0.95,
                "top_k": 64,
                "max_output_tokens": 8192,
                "response_mime_type": "text/plain",
            }
            safety_settings = [
                {
                    "category": "HARM_CATEGORY_HARASSMENT",
                    "threshold": "BLOCK_MEDIUM_AND_ABOVE",
                },
                {
                    "category": "HARM_CATEGORY_HATE_SPEECH",
                    "threshold": "BLOCK_MEDIUM_AND_ABOVE",
                },
                {
                    "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",
                    "threshold": "BLOCK_MEDIUM_AND_ABOVE",
                },
                {
                    "category": "HARM_CATEGORY_DANGEROUS_CONTENT",
                    "threshold": "BLOCK_MEDIUM_AND_ABOVE",
                },
            ]

            self.model = genai.GenerativeModel(
                model_name=model_name,
                safety_settings=safety_settings,
                generation_config=generation_config,
            )
            self.logger.info(
                f"Modelo Gemini '{self.model.model_name}' inicializado com sucesso para {self.__class__.__name__}."
            )

        except RuntimeError as rte:
            self.logger.critical(
                f"Falha na inicializaÃ§Ã£o do serviÃ§o (configuraÃ§Ã£o da API): {rte}"
            )
            raise
        except Exception as e:
            self.logger.critical(
                f"Falha catastrÃ³fica na inicializaÃ§Ã£o do serviÃ§o ({self.__class__.__name__}): {e}",
                exc_info=True,
            )
            self.model = None
            raise RuntimeError(
                f"Falha catastrÃ³fica na inicializaÃ§Ã£o do serviÃ§o de IA: {e}"
            ) from e

    def _generate_cache_key(self, prompt_final: str) -> str:
        """Gera uma chave de cache SHA256 para o prompt."""
        cache_key = hashlib.sha256(prompt_final.encode("utf-8")).hexdigest()
        self.logger.info(f"ğŸ”‘ Chave de cache gerada: {cache_key[:16]}...")
        return cache_key

    # APLICAÃ‡ÃƒO DO CACHE COM O DECORATOR @cache.memoize
    @cache.memoize(timeout=3600)  # Cache por 1 hora
    def _call_generative_model(self, prompt_final: str) -> str:
        import google.generativeai as genai
        import os
        
        # Log detalhado do cache
        cache_key = self._generate_cache_key(prompt_final)
        self.logger.info(f"ğŸš€ CACHE MISS - Nova consulta para chave: {cache_key[:16]}...")
        self.logger.info(f"ğŸ“ Prompt (primeiros 100 chars): {prompt_final[:100]}...")
        
        if not isinstance(prompt_final, str) or not prompt_final.strip():
            self.logger.warning("Prompt final estÃ¡ vazio ou nÃ£o Ã© uma string.")
            raise ValueError("Prompt final para a IA nÃ£o pode ser vazio.")

        # ConfiguraÃ§Ã£o das API Keys para fallback automÃ¡tico
        # IMPORTANTE: Configure no .env ou variÃ¡veis de ambiente:
        # - GOOGLE_API_KEY_1 (API Key principal)
        # - GOOGLE_API_KEY_2 (API Key de backup/fallback)
        api_keys = [
            os.environ.get("GOOGLE_API_KEY_1"),
            os.environ.get("GOOGLE_API_KEY_2")
        ]
        last_exception = None
        for idx, api_key in enumerate(api_keys, 1):
            if not api_key:
                self.logger.warning(f"GOOGLE_API_KEY_{idx} nÃ£o configurada, pulando...")
                continue
            try:
                genai.configure(api_key=api_key)
                self.logger.info(f"ğŸ”‘ Usando GOOGLE_API_KEY_{idx} para chamada Gemini.")
                # Recria o modelo para garantir que estÃ¡ usando a API Key correta
                generation_config = {
                    "temperature": 0.7,
                    "top_p": 0.95,
                    "top_k": 64,
                    "max_output_tokens": 8192,
                    "response_mime_type": "text/plain",
                }
                safety_settings = [
                    {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
                    {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
                    {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
                    {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
                ]
                model = genai.GenerativeModel(
                    model_name=getattr(self, 'model', None).model_name if getattr(self, 'model', None) else "gemini-1.5-flash-latest",
                    safety_settings=safety_settings,
                    generation_config=generation_config,
                )
                self.logger.info(f"ğŸ¤– Enviando prompt para o modelo Gemini (API {idx})")
                response = model.generate_content(prompt_final)
                self.logger.debug(f"Resposta bruta da API Gemini: {response}")

                if response.parts:
                    texto_processado = "".join(
                        part.text for part in response.parts if hasattr(part, "text")
                    )
                    if texto_processado:
                        self.logger.info(f"âœ… Resposta da IA processada com sucesso (via response.parts) - {len(texto_processado)} chars")
                        self.logger.info(f"ğŸ’¾ Salvando no cache com chave: {cache_key[:16]}...")
                        return texto_processado
                    elif (
                        response.prompt_feedback
                        and str(response.prompt_feedback.block_reason) != "BLOCK_REASON_UNSPECIFIED"
                    ):
                        block_reason_detail = (
                            response.prompt_feedback.block_reason_message
                            or str(response.prompt_feedback.block_reason)
                        )
                        self.logger.warning(f"ğŸš« ConteÃºdo bloqueado pela IA (detectado em parts). Motivo: {block_reason_detail}")
                        raise ValueError(f"O conteÃºdo gerado foi bloqueado pela IA. Motivo: {block_reason_detail}")
                    else:
                        self.logger.warning("âš ï¸ Resposta da IA (via parts) nÃ£o contÃ©m texto processado ou partes vÃ¡lidas.")
                        raise ValueError("Resposta da IA (via parts) estÃ¡ vazia ou invÃ¡lida.")

                elif hasattr(response, "text") and response.text:
                    self.logger.info(f"âœ… Resposta da IA processada com sucesso (via response.text) - {len(response.text)} chars")
                    self.logger.info(f"ğŸ’¾ Salvando no cache com chave: {cache_key[:16]}...")
                    return response.text

                elif (
                    response.prompt_feedback
                    and str(response.prompt_feedback.block_reason) != "BLOCK_REASON_UNSPECIFIED"
                ):
                    block_reason_detail = (
                        response.prompt_feedback.block_reason_message
                        or str(response.prompt_feedback.block_reason)
                    )
                    self.logger.warning(f"ğŸš« ConteÃºdo bloqueado pela IA (sem partes/texto, mas com feedback). Motivo: {block_reason_detail}")
                    raise ValueError(f"O conteÃºdo gerado foi bloqueado pela IA. Motivo: {block_reason_detail}")
                else:
                    self.logger.warning("âš ï¸ Resposta da API Gemini nÃ£o contÃ©m 'text', 'parts' vÃ¡lidas ou feedback de bloqueio claro.")
                    raise ValueError("Resposta da API Gemini estÃ¡ em formato inesperado ou vazia.")
            except Exception as e:
                self.logger.error(f"âŒ Erro ao tentar GOOGLE_API_KEY_{idx}: {e}", exc_info=True)
                last_exception = e
                continue
        
        # Se chegou aqui, todas as tentativas falharam
        if last_exception:
            raise RuntimeError(f"Todas as APIs Gemini falharam. Ãšltimo erro: {last_exception}")
        else:
            raise RuntimeError("Nenhuma API Key Gemini configurada. Configure GOOGLE_API_KEY_1 ou GOOGLE_API_KEY_2 no .env")

    def _call_generative_model_with_cache_logging(self, prompt_final: str) -> str:
        """VersÃ£o da funÃ§Ã£o com logs detalhados de cache."""
        cache_key = self._generate_cache_key(prompt_final)
        
        # Verifica se existe no cache
        try:
            cached_result = cache.get(cache_key)
            if cached_result:
                self.logger.info(f"ğŸ¯ CACHE HIT - Resposta encontrada no cache para chave: {cache_key[:16]}...")
                self.logger.info(f"ğŸ“Š Tamanho da resposta em cache: {len(cached_result)} chars")
                return cached_result
            else:
                self.logger.info(f"ğŸš€ CACHE MISS - Chave nÃ£o encontrada: {cache_key[:16]}...")
        except Exception as e:
            self.logger.warning(f"âš ï¸ Erro ao verificar cache: {e}")
        
        # Se nÃ£o estÃ¡ no cache, chama a funÃ§Ã£o original
        result = self._call_generative_model(prompt_final)
        self.logger.info(f"ğŸ’¾ Resposta salva no cache com chave: {cache_key[:16]}...")
        return result
