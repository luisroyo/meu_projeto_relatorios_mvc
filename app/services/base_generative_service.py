# app/services/base_generative_service.py
import hashlib  # <-- NOVA IMPORTA√á√ÉO para criar chaves de cache est√°veis
import logging
import os

import google.generativeai as genai

from app import cache  # <-- NOVA IMPORTA√á√ÉO


class BaseGenerativeService:
    def __init__(self, model_name="gemini-1.5-flash-latest"):
        self.logger = logging.getLogger(self.__class__.__name__)
        self.model = None
        self._google_api_key = None

        try:
            # Tenta usar GOOGLE_API_KEY_1 primeiro, depois GOOGLE_API_KEY como fallback
            # IMPORTANTE: Configure no .env ou vari√°veis de ambiente:
            # - GOOGLE_API_KEY_1 (API Key principal para inicializa√ß√£o)
            # - GOOGLE_API_KEY_2 (API Key de backup para fallback)
            self._google_api_key = os.getenv("GOOGLE_API_KEY_1") or os.getenv("GOOGLE_API_KEY")
            if not self._google_api_key:
                self.logger.error(
                    "API Key do Google (GOOGLE_API_KEY_1 ou GOOGLE_API_KEY) n√£o encontrada nas vari√°veis de ambiente."
                )
                raise RuntimeError(
                    "API Key do Google (GOOGLE_API_KEY_1 ou GOOGLE_API_KEY) n√£o configurada nas vari√°veis de ambiente."
                )

            genai.configure(api_key=self._google_api_key)
            self.logger.info(
                "Configura√ß√£o da API Key do Google bem-sucedida para o servi√ßo."
            )

            generation_config = {
                "temperature": 0.7,
                "top_p": 0.95,
                "top_k": 64,
                "max_output_tokens": 8192,
                "response_mime_type": "text/plain",
            }
            safety_settings = [
                {
                    "category": "HARM_CATEGORY_HARASSMENT",
                    "threshold": "BLOCK_MEDIUM_AND_ABOVE",
                },
                {
                    "category": "HARM_CATEGORY_HATE_SPEECH",
                    "threshold": "BLOCK_MEDIUM_AND_ABOVE",
                },
                {
                    "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",
                    "threshold": "BLOCK_MEDIUM_AND_ABOVE",
                },
                {
                    "category": "HARM_CATEGORY_DANGEROUS_CONTENT",
                    "threshold": "BLOCK_MEDIUM_AND_ABOVE",
                },
            ]

            self.model = genai.GenerativeModel(
                model_name=model_name,
                safety_settings=safety_settings,
                generation_config=generation_config,
            )
            self.logger.info(
                f"Modelo Gemini '{self.model.model_name}' inicializado com sucesso para {self.__class__.__name__}."
            )

        except RuntimeError as rte:
            self.logger.critical(
                f"Falha na inicializa√ß√£o do servi√ßo (configura√ß√£o da API): {rte}"
            )
            raise
        except Exception as e:
            self.logger.critical(
                f"Falha catastr√≥fica na inicializa√ß√£o do servi√ßo ({self.__class__.__name__}): {e}",
                exc_info=True,
            )
            self.model = None
            raise RuntimeError(
                f"Falha catastr√≥fica na inicializa√ß√£o do servi√ßo de IA: {e}"
            ) from e

    def _generate_cache_key(self, prompt_final: str) -> str:
        """Gera uma chave de cache SHA256 para o prompt."""
        cache_key = hashlib.sha256(prompt_final.encode("utf-8")).hexdigest()
        self.logger.info(f"üîë Chave de cache gerada: {cache_key[:16]}...")
        return cache_key

    # APLICA√á√ÉO DO CACHE COM O DECORATOR @cache.memoize
    @cache.memoize(timeout=3600)  # Cache por 1 hora
    def _call_generative_model(self, prompt_final: str) -> str:
        import google.generativeai as genai
        import os
        
        # Log detalhado do cache
        cache_key = self._generate_cache_key(prompt_final)
        self.logger.info(f"üöÄ CACHE MISS - Nova consulta para chave: {cache_key[:16]}...")
        self.logger.info(f"üìù Prompt (primeiros 100 chars): {prompt_final[:100]}...")
        
        if not isinstance(prompt_final, str) or not prompt_final.strip():
            self.logger.warning("Prompt final est√° vazio ou n√£o √© uma string.")
            raise ValueError("Prompt final para a IA n√£o pode ser vazio.")

        # Configura√ß√£o das API Keys para fallback autom√°tico
        # IMPORTANTE: Configure no .env ou vari√°veis de ambiente:
        # - GOOGLE_API_KEY_1 (API Key principal)
        # - GOOGLE_API_KEY_2 (API Key de backup/fallback)
        api_keys = [
            os.environ.get("GOOGLE_API_KEY_1"),
            os.environ.get("GOOGLE_API_KEY_2")
        ]
        last_exception = None
        for idx, api_key in enumerate(api_keys, 1):
            if not api_key:
                self.logger.warning(f"GOOGLE_API_KEY_{idx} n√£o configurada, pulando...")
                continue
            try:
                genai.configure(api_key=api_key)
                self.logger.info(f"üîë Usando GOOGLE_API_KEY_{idx} para chamada Gemini.")
                # Recria o modelo para garantir que est√° usando a API Key correta
                generation_config = {
                    "temperature": 0.7,
                    "top_p": 0.95,
                    "top_k": 64,
                    "max_output_tokens": 8192,
                    "response_mime_type": "text/plain",
                }
                safety_settings = [
                    {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
                    {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
                    {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
                    {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
                ]
                model = genai.GenerativeModel(
                    model_name=getattr(self, 'model', None).model_name if getattr(self, 'model', None) else "gemini-1.5-flash-latest",
                    safety_settings=safety_settings,
                    generation_config=generation_config,
                )
                self.logger.info(f"ü§ñ Enviando prompt para o modelo Gemini (API {idx})")
                response = model.generate_content(prompt_final)
                self.logger.debug(f"Resposta bruta da API Gemini: {response}")

                if response.parts:
                    texto_processado = "".join(
                        part.text for part in response.parts if hasattr(part, "text")
                    )
                    if texto_processado:
                        self.logger.info(f"‚úÖ Resposta da IA processada com sucesso (via response.parts) - {len(texto_processado)} chars")
                        self.logger.info(f"üíæ Salvando no cache com chave: {cache_key[:16]}...")
                        return texto_processado
                    elif (
                        response.prompt_feedback
                        and str(response.prompt_feedback.block_reason) != "BLOCK_REASON_UNSPECIFIED"
                    ):
                        block_reason_detail = (
                            response.prompt_feedback.block_reason_message
                            or str(response.prompt_feedback.block_reason)
                        )
                        self.logger.warning(f"üö´ Conte√∫do bloqueado pela IA (detectado em parts). Motivo: {block_reason_detail}")
                        raise ValueError(f"O conte√∫do gerado foi bloqueado pela IA. Motivo: {block_reason_detail}")
                    else:
                        self.logger.warning("‚ö†Ô∏è Resposta da IA (via parts) n√£o cont√©m texto processado ou partes v√°lidas.")
                        raise ValueError("Resposta da IA (via parts) est√° vazia ou inv√°lida.")

                elif hasattr(response, "text") and response.text:
                    self.logger.info(f"‚úÖ Resposta da IA processada com sucesso (via response.text) - {len(response.text)} chars")
                    self.logger.info(f"üíæ Salvando no cache com chave: {cache_key[:16]}...")
                    return response.text

                elif (
                    response.prompt_feedback
                    and str(response.prompt_feedback.block_reason) != "BLOCK_REASON_UNSPECIFIED"
                ):
                    block_reason_detail = (
                        response.prompt_feedback.block_reason_message
                        or str(response.prompt_feedback.block_reason)
                    )
                    self.logger.warning(f"üö´ Conte√∫do bloqueado pela IA (sem partes/texto, mas com feedback). Motivo: {block_reason_detail}")
                    raise ValueError(f"O conte√∫do gerado foi bloqueado pela IA. Motivo: {block_reason_detail}")
                else:
                    self.logger.warning("‚ö†Ô∏è Resposta da API Gemini n√£o cont√©m 'text', 'parts' v√°lidas ou feedback de bloqueio claro.")
                    raise ValueError("Resposta da API Gemini est√° em formato inesperado ou vazia.")
            except Exception as e:
                self.logger.error(f"‚ùå Erro ao tentar GOOGLE_API_KEY_{idx}: {e}", exc_info=True)
                last_exception = e
                continue
        
        # Se chegou aqui, todas as tentativas falharam
        if last_exception:
            raise RuntimeError(f"Todas as APIs Gemini falharam. √öltimo erro: {last_exception}")
        else:
            raise RuntimeError("Nenhuma API Key Gemini configurada. Configure GOOGLE_API_KEY_1 ou GOOGLE_API_KEY_2 no .env")

    def _call_generative_model_with_cache_logging(self, prompt_final: str) -> str:
        """Vers√£o da fun√ß√£o com logs detalhados de cache."""
        cache_key = self._generate_cache_key(prompt_final)
        
        # Verifica se existe no cache
        try:
            cached_result = cache.get(cache_key)
            if cached_result:
                self.logger.info(f"üéØ CACHE HIT - Resposta encontrada no cache para chave: {cache_key[:16]}...")
                self.logger.info(f"üìä Tamanho da resposta em cache: {len(cached_result)} chars")
                return cached_result
            else:
                self.logger.info(f"üöÄ CACHE MISS - Chave n√£o encontrada: {cache_key[:16]}...")
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Erro ao verificar cache: {e}")
        
        # Se n√£o est√° no cache, chama a fun√ß√£o original
        result = self._call_generative_model(prompt_final)
        self.logger.info(f"üíæ Resposta salva no cache com chave: {cache_key[:16]}...")
        return result
